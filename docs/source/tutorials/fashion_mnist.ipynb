{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with PyTorch and Cleanlab\n",
    "\n",
    "This quickstart tutorial demonstrates how to find issues in image classification data. Here we use the Fashion-MNIST dataset containing 70,000 images of fashion products from 10 categories, with 7,000 images per category.\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Build a simple [PyTorch](https://pytorch.org/) neural net.\n",
    "\n",
    "- Use cross validation to compute out-of-sample predicted probabilities (`pred_probs`) and feature embeddings (`features`).\n",
    "\n",
    "- Utilize these `pred_probs` and `features` to identify potential issues within the dataset using the `Datalab` class from cleanlab. The issues found by cleanlab include mislabeled examples, near duplicates, outliers, and image-specific problems such as excessively dark or low information images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Quickstart\n",
    "<br/>\n",
    "    \n",
    "Already have a `model`? Run cross-validation to get out-of-sample `pred_probs` and `features` and then use the code below to find any potential issues in your dataset.\n",
    "\n",
    "\n",
    "<div  class=markdown markdown=\"1\" style=\"background:white;margin:16px\">  \n",
    "    \n",
    "```python\n",
    "from cleanlab import Datalab\n",
    "\n",
    "lab = Datalab(data=your_dataset, label_name=\"column_name_of_labels\")\n",
    "lab.find_issues(pred_probs=pred_probs, features=features)\n",
    "\n",
    "lab.report()\n",
    "```\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and import required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `pip` to install all packages required for this tutorial as follows:\n",
    "\n",
    "```ipython3\n",
    "!pip install matplotlib torch torchvision datasets\n",
    "!pip install \"cleanlab[datalab,image]\"\n",
    "# Make sure to install the version corresponding to this tutorial\n",
    "# E.g. if viewing master branch documentation:\n",
    "#     !pip install \"cleanlab[datalab,image] @ git+https://github.com/cleanlab/cleanlab.git\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Package installation (this cell is hidden from docs.cleanlab.ai).\n",
    "# If running on Colab, may want to use GPU (select: Runtime > Change runtime type > Hardware accelerator > GPU)\n",
    "\n",
    "dependencies = [\"cleanlab\", \"matplotlib\", \"torch\", \"torchvision\", \"datasets\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install cleanlab  # for colab\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")\n",
    "\n",
    "# Suppress benign warnings: \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", \"Lazy modules are a new feature.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from cleanlab import Datalab\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch and normalize the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(\"fashion_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View different splits of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test splits\n",
    "dataset = concatenate_datasets(list(dataset_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the number of rows and columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PIL image to torch tensors\n",
    "transformed_dataset = dataset.with_format(\"torch\")\n",
    "\n",
    "\n",
    "# Apply transformations\n",
    "def normalize(example):\n",
    "    example[\"image\"] = (example[\"image\"] / 255.0).unsqueeze(0)\n",
    "    return example\n",
    "\n",
    "\n",
    "transformed_dataset = transformed_dataset.map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Bringing Your Own Data (BYOD)?\n",
    "\n",
    "Load any huggingface dataset or your local imagefolder dataset, apply relevant transformations, and continue with the rest of the tutorial.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a classification model\n",
    "Here, we define a simple neural network with PyTorch. Note this is just a toy model to ensure quick runtimes for the tutorial, you can replace it with any other (larger) PyTorch network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.LazyLinear(120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def embeddings(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# This (optional) cell is hidden from docs.cleanlab.ai\n",
    "\n",
    "SEED = 123  # for reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Helper methods for cross validation **(click to expand)**</summary>\n",
    "\n",
    "```python\n",
    "# Note: This pulldown content is for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Method to calculate validation accuracy in each epoch\n",
    "def get_test_accuracy(net, testloader):\n",
    "    net.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = net(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = 100 * accuracy / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Method for training the model\n",
    "def train(trainloader, testloader, n_epochs, patience):\n",
    "    model = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_test_accuracy = 0.0\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for _, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        accuracy = get_test_accuracy(model, testloader)\n",
    "        print(\n",
    "            f\"epoch: {epoch + 1} loss: {running_loss / len(trainloader):.3f} test acc: {accuracy}\"\n",
    "        )\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if accuracy > best_test_accuracy:\n",
    "            best_epoch = epoch\n",
    "\n",
    "        if epoch - best_epoch > patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "# Method for computing out-of-sample embeddings\n",
    "def compute_embeddings(model, testloader):\n",
    "    embeddings_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            embeddings = model.embeddings(images)\n",
    "            embeddings_list.append(embeddings.cpu())\n",
    "\n",
    "    return torch.vstack(embeddings_list)\n",
    "\n",
    "\n",
    "# Method for computing out-of-sample predicted probabilities\n",
    "def compute_pred_probs(model, testloader):\n",
    "    pred_probs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            pred_probs_list.append(outputs.cpu())\n",
    "\n",
    "    return torch.vstack(pred_probs_list)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Method to calculate validation accuracy in each epoch\n",
    "def get_test_accuracy(net, testloader):\n",
    "    net.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = net(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = 100 * accuracy / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Method for training the model\n",
    "def train(trainloader, testloader, n_epochs, patience):\n",
    "    model = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_test_accuracy = 0.0\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for _, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        accuracy = get_test_accuracy(model, testloader)\n",
    "        print(\n",
    "            f\"epoch: {epoch + 1} loss: {running_loss / len(trainloader):.3f} test acc: {accuracy}\"\n",
    "        )\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if accuracy > best_test_accuracy:\n",
    "            best_epoch = epoch\n",
    "\n",
    "        if epoch - best_epoch > patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "# Method for computing out-of-sample embeddings\n",
    "def compute_embeddings(model, testloader):\n",
    "    embeddings_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            embeddings = model.embeddings(images)\n",
    "            embeddings_list.append(embeddings.cpu())\n",
    "\n",
    "    return torch.vstack(embeddings_list)\n",
    "\n",
    "\n",
    "# Method for computing out-of-sample predicted probabilities\n",
    "def compute_pred_probs(model, testloader):\n",
    "    pred_probs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            pred_probs_list.append(outputs.cpu())\n",
    "\n",
    "    return torch.vstack(pred_probs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create K folds of the dataset for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # Number of cross-validation folds. Set to small value here to ensure quick runtimes, we recommend 5 or 10 in practice for more accurate estimates.\n",
    "n_epochs = 3  # Number of epochs to train model for. Set to a small value here for quick runtime, you should use a larger value in practice.\n",
    "patience = 2  # Parameter for early stopping. If the validation accuracy does not improve for this many epochs, training will stop.\n",
    "batch_size = 4  # Batch size for training and testing\n",
    "\n",
    "kfold = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "splits = kfold.split(transformed_dataset)\n",
    "\n",
    "train_id_list, test_id_list = [], []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(splits):\n",
    "    train_id_list.append(train_ids)\n",
    "    test_id_list.append(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute out-of-sample predicted probabilities and feature embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_list, embeddings_list = [], []\n",
    "\n",
    "for i in range(K):\n",
    "    print(f\"Training on fold: {i+1}\")\n",
    "\n",
    "    # Create train and test sets and corresponding dataloaders\n",
    "    trainset = transformed_dataset.select(train_id_list[i])\n",
    "    testset = transformed_dataset.select(test_id_list[i])\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Train model\n",
    "    model = train(trainloader, testloader, n_epochs, patience)\n",
    "\n",
    "    # Compute out-of-sample embeddings\n",
    "    print(\"\\nComputing feature embeddings...\")\n",
    "    fold_embeddings = compute_embeddings(model, testloader)\n",
    "    embeddings_list.append(fold_embeddings)\n",
    "\n",
    "    print(\"\\nComputing predicted probabilities...\")\n",
    "    # Compute out-of-sample predicted probabilities\n",
    "    fold_pred_probs = compute_pred_probs(model, testloader)\n",
    "    pred_probs_list.append(fold_pred_probs)\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "\n",
    "# Combine embeddings and predicted probabilities from each fold\n",
    "features = torch.vstack(embeddings_list).numpy()\n",
    "\n",
    "logits = torch.vstack(pred_probs_list)\n",
    "pred_probs = nn.Softmax(dim=1)(logits).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder rows of the dataset based on row-order in `features` and `pred_probs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.hstack(test_id_list)\n",
    "dataset = dataset.select(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use cleanlab to find issues\n",
    "\n",
    "Based on the given labels, out-of-sample predicted probabilities and feature embeddings, cleanlab can quickly help us detect potential issues in our dataset. \n",
    "\n",
    "Here, we use cleanlab's `Datalab` class to find potential issues in our data. `Datalab` has several ways of loading the data. For our current scenario, we are employing the huggingface dataset. `Datalab` takes in two optional arguments, `label_name`, which corresponds to the label column's name, and `image_key`, corresponding to the image column's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = Datalab(data=dataset, label_name=\"label\", image_key=\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_issues` method can automatically infer the types of issues to be checked for based on the provided arguments. Here, we provide `features` and `pred_probs` as arguments. If you want to check for a specific issue type, you can do so using the `issue_types` argument. Check the [documentation](https://docs.cleanlab.ai/stable/cleanlab/datalab/datalab.html#cleanlab.datalab.datalab.Datalab.find_issues) for more comprehensive guide on `find_issues` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.find_issues(features=features, pred_probs=pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View report\n",
    "\n",
    "After the audit is complete, we can view the results and information regarding the issues using `report` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.report(include_description=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label issues\n",
    "\n",
    "Let's first inspect mislabeled examples in the dataset. Such errors occur when the given label for an image is incorrect, usually due to mistakes made by data annotators. Cleanlab automatically detects  mislabeled data that you can correct to improve your dataset.\n",
    "\n",
    "`get_issues` method returns all the information related to the issue specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_issues = lab.get_issues(\"label\")\n",
    "label_issues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dataframe contains a `label_score` for each example in the dataset. These numeric quality scores lie between 0 and 1, where lower scores indicate examples more likely to be mislabeled. It contains a boolean column `is_label_issue` specifying whether or not each example appears to have a label issue (indicating it is likely mislabeled).\n",
    "\n",
    "We filter the `label_issues` DataFrame to find examples that are considered to have label errors and sort by `label_score` in ascending order to get the most severe mislabeled examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_issues_df = label_issues.query(\"is_label_issue\").sort_values(\"label_score\")\n",
    "label_issues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>See the implementation of `plot_label_issue_examples` **(click to expand)**</summary>\n",
    "\n",
    "```python\n",
    "# Note: This pulldown content is for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "def plot_label_issue_examples(label_issues_df, num_examples=15):\n",
    "    ncols = 5\n",
    "    nrows = int(math.ceil(num_examples / ncols))\n",
    "\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    axes_list = axes.flatten()\n",
    "    label_issue_indices = label_issues_df.index.values\n",
    "\n",
    "    for i, ax in enumerate(axes_list):\n",
    "        if i  >= num_examples:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        idx = int(label_issue_indices[i])\n",
    "        row = label_issues.loc[idx]\n",
    "        ax.set_title(\n",
    "            f\"id: {idx}\\n GL: {row.given_label}\\n SL: {row.predicted_label}\",\n",
    "            fontdict={\"fontsize\": 8},\n",
    "        )\n",
    "        ax.imshow(dataset[idx][\"image\"], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(h_pad=2.0)\n",
    "    plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def plot_label_issue_examples(label_issues_df, num_examples=15):\n",
    "    ncols = 5\n",
    "    nrows = int(math.ceil(num_examples / ncols))\n",
    "\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    axes_list = axes.flatten()\n",
    "    label_issue_indices = label_issues_df.index.values\n",
    "\n",
    "    for i, ax in enumerate(axes_list):\n",
    "        if i >= num_examples:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        idx = int(label_issue_indices[i])\n",
    "        row = label_issues.loc[idx]\n",
    "        ax.set_title(\n",
    "            f\"id: {idx}\\n GL: {row.given_label}\\n SL: {row.predicted_label}\",\n",
    "            fontdict={\"fontsize\": 8},\n",
    "        )\n",
    "        ax.imshow(dataset[idx][\"image\"], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(h_pad=2.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View most likely examples with label errors\n",
    "\n",
    "Here we define\n",
    "`GL` : given label in the original dataset\n",
    "`SL` : suggested alternative label by cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_issue_examples(label_issues_df, num_examples=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Outlier issues\n",
    "\n",
    "In this section, we focus on reviewing atypical examples in the dataset. These are examples that are significantly different from the majority of the dataset, which may have an outsized impact on models fit to this data.\n",
    "\n",
    "Similarly to the previous section, we filter the `outlier_issues` DataFrame to find examples that are considered to be outliers. We then sort the filtered results by their outlier quality score in ascending order, where examples with the lowest scores appear least typical based on the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_issues_df = lab.get_issues(\"outlier\")\n",
    "outlier_issues_df = outlier_issues_df.query(\"is_outlier_issue\").sort_values(\"outlier_score\")\n",
    "outlier_issues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View most likely outliers\n",
    "\n",
    "In this visualization, the first image in every row shows the potential outlier, while the remaining images in the same row depict typical instances from the corresponding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>See the implementation of `plot_outlier_issues_examples` **(click to expand)**</summary>\n",
    "\n",
    "```python\n",
    "# Note: This pulldown content is for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "def plot_outlier_issues_examples(outlier_issues_df, num_examples):\n",
    "    ncols=4\n",
    "    nrows = num_examples\n",
    "    N_comparison_images = ncols - 1\n",
    "\n",
    "    def sample_from_class(label, number_of_samples, index):\n",
    "        index = int(index)\n",
    "\n",
    "        non_outlier_indices = (\n",
    "            label_issues.join(outlier_issues_df)\n",
    "            .query(\"given_label == @label and is_outlier_issue.isnull()\")\n",
    "            .index\n",
    "        )\n",
    "        non_outlier_indices_excluding_current = non_outlier_indices[non_outlier_indices != index]\n",
    "\n",
    "        sampled_indices = np.random.choice(\n",
    "            non_outlier_indices_excluding_current, number_of_samples, replace=False\n",
    "        )\n",
    "\n",
    "        label_scores_of_sampled = label_issues.loc[sampled_indices][\"label_score\"]\n",
    "\n",
    "        top_score_indices = np.argsort(label_scores_of_sampled.values)[::-1][:N_comparison_images]\n",
    "\n",
    "        top_label_indices = sampled_indices[top_score_indices]\n",
    "\n",
    "        sampled_images = [dataset[int(i)][\"image\"] for i in top_label_indices]\n",
    "\n",
    "        return sampled_images\n",
    "\n",
    "    def get_image_given_label_and_samples(idx):\n",
    "        image_from_dataset = dataset[idx][\"image\"]\n",
    "        corresponding_label = label_issues.loc[idx][\"given_label\"]\n",
    "        comparison_images = sample_from_class(corresponding_label, 30, idx)[:N_comparison_images]\n",
    "\n",
    "        return image_from_dataset, corresponding_label, comparison_images\n",
    "\n",
    "    count = 0\n",
    "    for idx, row in outlier_issues_df.iterrows():\n",
    "        idx = row.name\n",
    "        image, label, comparison_images = get_image_given_label_and_samples(idx)\n",
    "        image_to_plot = [image] + comparison_images\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=len(image_to_plot))\n",
    "        for i, ax in enumerate(axes):\n",
    "            if i == 0:\n",
    "                ax.set_title(f\"id: {idx}\\n GL: {label}\", fontdict={\"fontsize\": 8})\n",
    "            ax.imshow(image_to_plot[i], cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        count += 1\n",
    "        if count >= nrows:\n",
    "            break\n",
    "    plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def plot_outlier_issues_examples(outlier_issues_df, num_examples):\n",
    "    ncols = 4\n",
    "    nrows = num_examples\n",
    "    N_comparison_images = ncols - 1\n",
    "\n",
    "    def sample_from_class(label, number_of_samples, index):\n",
    "        index = int(index)\n",
    "\n",
    "        non_outlier_indices = (\n",
    "            label_issues.join(outlier_issues_df)\n",
    "            .query(\"given_label == @label and is_outlier_issue.isnull()\")\n",
    "            .index\n",
    "        )\n",
    "        non_outlier_indices_excluding_current = non_outlier_indices[non_outlier_indices != index]\n",
    "\n",
    "        sampled_indices = np.random.choice(\n",
    "            non_outlier_indices_excluding_current, number_of_samples, replace=False\n",
    "        )\n",
    "\n",
    "        label_scores_of_sampled = label_issues.loc[sampled_indices][\"label_score\"]\n",
    "\n",
    "        top_score_indices = np.argsort(label_scores_of_sampled.values)[::-1][:N_comparison_images]\n",
    "\n",
    "        top_label_indices = sampled_indices[top_score_indices]\n",
    "\n",
    "        sampled_images = [dataset[int(i)][\"image\"] for i in top_label_indices]\n",
    "\n",
    "        return sampled_images\n",
    "\n",
    "    def get_image_given_label_and_samples(idx):\n",
    "        image_from_dataset = dataset[idx][\"image\"]\n",
    "        corresponding_label = label_issues.loc[idx][\"given_label\"]\n",
    "        comparison_images = sample_from_class(corresponding_label, 30, idx)[:N_comparison_images]\n",
    "\n",
    "        return image_from_dataset, corresponding_label, comparison_images\n",
    "\n",
    "    count = 0\n",
    "    for idx, row in outlier_issues_df.iterrows():\n",
    "        idx = row.name\n",
    "        image, label, comparison_images = get_image_given_label_and_samples(idx)\n",
    "        image_to_plot = [image] + comparison_images\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=len(image_to_plot))\n",
    "        for i, ax in enumerate(axes):\n",
    "            if i == 0:\n",
    "                ax.set_title(f\"id: {idx}\\n GL: {label}\", fontdict={\"fontsize\": 8})\n",
    "            ax.imshow(image_to_plot[i], cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        count += 1\n",
    "        if count >= nrows:\n",
    "            break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outlier_issues_examples(outlier_issues_df, num_examples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View near duplicate issues\n",
    "\n",
    "In this section, we display examples from the dataset that are considered to be nearly duplicated. A lot of near duplicate images in the dataset can lead to model overfitting, It can also impact the accuracy of metrics, subsequently affecting the process of model selection.\n",
    "\n",
    "The `near_duplicate_issues` DataFrame tells use which examples are considered to have near duplicates, and we can sort them via the `near_duplicate_score` which quantifies how severe this issue is for each image (lower values indicate more severe intances of a type of issue).\n",
    "\n",
    "This allows us to visualize examples in the dataset that are considered near duplicates, along with their near-duplicate counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_duplicate_issues_df = lab.get_issues(\"near_duplicate\")\n",
    "near_duplicate_issues_df = near_duplicate_issues_df.query(\"is_near_duplicate_issue\").sort_values(\n",
    "    \"near_duplicate_score\"\n",
    ")\n",
    "near_duplicate_issues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View sets of near duplicate images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>See the implementation of `plot_near_duplicate_issue_examples` **(click to expand)**</summary>\n",
    "\n",
    "```python\n",
    "# Note: This pulldown content is for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "def plot_near_duplicate_issue_examples(near_duplicate_issues_df, num_examples=3):\n",
    "    nrows = num_examples\n",
    "    seen_id_pairs = set()\n",
    "\n",
    "    def get_image_and_given_label_and_predicted_label(idx):\n",
    "        image = dataset[idx][\"image\"]\n",
    "        label = label_issues.loc[idx][\"given_label\"]\n",
    "        predicted_label = label_issues.loc[idx][\"predicted_label\"]\n",
    "        return image, label, predicted_label\n",
    "\n",
    "    count = 0\n",
    "    for idx, row in near_duplicate_issues_df.iterrows():\n",
    "        image, label, predicted_label = get_image_and_given_label_and_predicted_label(idx)\n",
    "        duplicate_images = row.near_duplicate_sets\n",
    "        nd_set = set([int(i) for i in duplicate_images])\n",
    "        nd_set.add(int(idx))\n",
    "\n",
    "        if nd_set & seen_id_pairs:\n",
    "            continue\n",
    "\n",
    "        _, axes = plt.subplots(1, len(nd_set), figsize=(len(nd_set), 3))\n",
    "        for i, ax in zip(list(nd_set), axes):\n",
    "            label = label_issues.loc[i][\"given_label\"]\n",
    "            ax.set_title(f\"id: {i}\\n GL: {label}\", fontdict={\"fontsize\": 8})\n",
    "            ax.imshow(dataset[i][\"image\"], cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        seen_id_pairs.update(nd_set)\n",
    "        count += 1\n",
    "        if count >= nrows:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def plot_near_duplicate_issue_examples(near_duplicate_issues_df, num_examples=3):\n",
    "    nrows = num_examples\n",
    "    seen_id_pairs = set()\n",
    "\n",
    "    def get_image_and_given_label_and_predicted_label(idx):\n",
    "        image = dataset[idx][\"image\"]\n",
    "        label = label_issues.loc[idx][\"given_label\"]\n",
    "        predicted_label = label_issues.loc[idx][\"predicted_label\"]\n",
    "        return image, label, predicted_label\n",
    "\n",
    "    count = 0\n",
    "    for idx, row in near_duplicate_issues_df.iterrows():\n",
    "        image, label, predicted_label = get_image_and_given_label_and_predicted_label(idx)\n",
    "        duplicate_images = row.near_duplicate_sets\n",
    "        nd_set = set([int(i) for i in duplicate_images])\n",
    "        nd_set.add(int(idx))\n",
    "\n",
    "        if nd_set & seen_id_pairs:\n",
    "            continue\n",
    "\n",
    "        _, axes = plt.subplots(1, len(nd_set), figsize=(len(nd_set), 3))\n",
    "        for i, ax in zip(list(nd_set), axes):\n",
    "            label = label_issues.loc[i][\"given_label\"]\n",
    "            ax.set_title(f\"id: {i}\\n GL: {label}\", fontdict={\"fontsize\": 8})\n",
    "            ax.imshow(dataset[i][\"image\"], cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        seen_id_pairs.update(nd_set)\n",
    "        count += 1\n",
    "        if count >= nrows:\n",
    "            break\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_near_duplicate_issue_examples(near_duplicate_issues_df, num_examples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View dark images\n",
    "\n",
    "In this section, we display examples from the dataset that are too dark. Sometimes, when the images are too dark, it becomes challenging for both annotators and model to assign\n",
    "then into one class, leading to poor quality data for training and testing.\n",
    "\n",
    "The `dark_issues` DataFrame tells use which examples are considered to be too dark, and we can sort them via the `dark_score` which quantifies how severe this issue is for each image (lower values indicate more severe intances of a type of issue). This allows us to visualize examples in the dataset that are considered to be too dark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_issues = lab.get_issues(\"dark\")\n",
    "dark_issues_df = dark_issues.query(\"is_dark_issue\").sort_values(\"dark_score\")\n",
    "dark_issues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View top examples of dark images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>See the implementation of `plot_image_issue_examples` **(click to expand)**</summary>\n",
    "\n",
    "```python\n",
    "# Note: This pulldown content is for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "def plot_image_issue_examples(issues_df, num_examples=5):\n",
    "    ncols = 5\n",
    "    nrows = int(math.ceil(num_examples / ncols))\n",
    "\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    axes_list = axes.flatten()\n",
    "    issue_indices = issues_df.index.values\n",
    "\n",
    "    for i, ax in enumerate(axes_list):\n",
    "        if i  >= num_examples:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        idx = int(issue_indices[i])\n",
    "        label = label_issues.loc[idx][\"given_label\"]\n",
    "        predicted_label = label_issues.loc[idx][\"predicted_label\"]\n",
    "        ax.set_title(\n",
    "            f\"id: {idx}\\n GL: {label}\\n SL: {predicted_label}\",\n",
    "            fontdict={\"fontsize\": 8},\n",
    "        )\n",
    "        ax.imshow(dataset[idx][\"image\"], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(h_pad=2.0)\n",
    "    plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def plot_image_issue_examples(issues_df, num_examples=15):\n",
    "    ncols = 5\n",
    "    nrows = int(math.ceil(num_examples / ncols))\n",
    "\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    axes_list = axes.flatten()\n",
    "    issue_indices = issues_df.index.values\n",
    "\n",
    "    for i, ax in enumerate(axes_list):\n",
    "        if i >= num_examples:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        idx = int(issue_indices[i])\n",
    "        label = label_issues.loc[idx][\"given_label\"]\n",
    "        predicted_label = label_issues.loc[idx][\"predicted_label\"]\n",
    "        ax.set_title(\n",
    "            f\"id: {idx}\\n GL: {label}\\n SL: {predicted_label}\",\n",
    "            fontdict={\"fontsize\": 8},\n",
    "        )\n",
    "        ax.imshow(dataset[idx][\"image\"], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(h_pad=2.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_issue_examples(dark_issues_df, num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above examples that too dark images can also lead to label errors as it is difficult to see the contents of the image clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View low information images\n",
    "\n",
    "In this section, we display examples from the dataset that are low information. Low information images can impact the quality of the dataset and can also impact the generalizability of the model if they are present disproportionately in one class.\n",
    "\n",
    "The `lowinfo_issues` DataFrame tells use which examples are considered to be low information, and we can sort them via the `low_information_score` which quantifies how severe this issue is for each image (lower values indicate more severe intances of a type of issue). This allows us to visualize examples in the dataset which are most severe examples of low information issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowinfo_issues = lab.get_issues(\"low_information\")\n",
    "lowinfo_issues_df = lowinfo_issues.query(\"is_low_information_issue\").sort_values(\n",
    "    \"low_information_score\"\n",
    ")\n",
    "lowinfo_issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_issue_examples(lowinfo_issues_df, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Note: This cell is only for docs.cleanlab.ai, if running on local Jupyter or Colab, please ignore it.\n",
    "\n",
    "issue_indices = (\n",
    "    label_issues_df.index.values.tolist()\n",
    "    + outlier_issues_df.index.values.tolist()\n",
    "    + near_duplicate_issues_df.index.values.tolist()\n",
    "    + dark_issues_df.index.values.tolist()\n",
    "    + lowinfo_issues_df.index.values.tolist()\n",
    ")\n",
    "issue_indices = list(set(issue_indices))\n",
    "\n",
    "# low info, dark, near duplicate, outlier, label in order\n",
    "highlighted_indices = (\n",
    "    [12908] + [21432, 21612] + [53554, 58806] + [6547, 52321] + [58039, 55812]\n",
    ")  # verify these examples were found by cleanlab\n",
    "\n",
    "if not all(x in issue_indices for x in highlighted_indices):\n",
    "    raise Exception(\"Some highlighted examples are missing from ranked_label_issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
